{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This document analyzes the Rate My Hydrograph ratings of the second round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import itertools\n",
    "import xarray\n",
    "from pathlib import Path\n",
    "from neuralhydrology.evaluation.metrics import kge, nse, fdc_fhv, fdc_flv\n",
    "from sklearn import metrics\n",
    "import tqdm\n",
    "\n",
    "df = pd.read_csv('data/rmh-stage2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(589, 29)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at chains of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:01<00:00, 177.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triangles: 2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "cycles = []\n",
    "id_df = df.set_index('id')\n",
    "groups = df.groupby(['basin', 'start_date', 'objective', 'task'])\n",
    "for k in tqdm(groups.groups):\n",
    "    group = groups.get_group(k)\n",
    "    graph = nx.from_pandas_edgelist(group, source='model_a', target='model_b', edge_key='id', edge_attr=True, create_using=nx.MultiGraph)\n",
    "    cliques = nx.enumerate_all_cliques(graph)\n",
    "    triad_cliques = [x for x in cliques if len(x) == 3]\n",
    "    candidates = []\n",
    "\n",
    "    # Bring the entries of each triangle into a consistent order\n",
    "    for cycle in triad_cliques:\n",
    "        for edge_key, edge_attrs in graph.get_edge_data(cycle[0], cycle[1]).items():\n",
    "            model_a = id_df.loc[edge_key, 'model_a']\n",
    "            model_b = id_df.loc[edge_key, 'model_b']\n",
    "            for edge_key_2, edge_attrs_2 in graph.get_edge_data(cycle[1], cycle[2]).items():\n",
    "                model_c = id_df.loc[edge_key_2, 'model_a']\n",
    "                if model_c in [model_a, model_b]:\n",
    "                    model_c = id_df.loc[edge_key_2, 'model_b']\n",
    "\n",
    "                for edge_key_3, edge_attrs_3 in graph.get_edge_data(cycle[2], cycle[0]).items():\n",
    "                    candidate = [edge_attrs['id'], edge_attrs_2['id'], edge_attrs_3['id']]\n",
    "\n",
    "                    if id_df.loc[edge_key_2, 'model_a'] == model_a or id_df.loc[edge_key_2, 'model_b'] == model_a:\n",
    "                        # edge 2 is an ac edge, not a bc edge\n",
    "                        candidate[1], candidate[2] = candidate[2], candidate[1]\n",
    "                \n",
    "                    need_switch_2 = id_df.loc[candidate[1], 'model_a'] != model_b\n",
    "                    need_switch_3 = id_df.loc[candidate[2], 'model_a'] != model_c\n",
    "\n",
    "                    candidates.append((candidate[0], (candidate[1], need_switch_2), (candidate[2], need_switch_3)))\n",
    "    cycles += candidates\n",
    "print(f'Number of triangles: {len(cycles)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count consistent/inconsistent triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistent: 786, conflict: 109, unclear: 0, eq consistent: 680, eq conflict: 405, 3-eq consistent: 192, 2-eq conflict: 335. Total: 2507 (should be 2507)\n",
      "Consistent total: 1658, conflict total: 514, unclear: 335.\n"
     ]
    }
   ],
   "source": [
    "consistent = 0\n",
    "eq_consistent = 0\n",
    "triple_eq_consistent = 0\n",
    "unclear = 0\n",
    "eq_conflict = 0\n",
    "double_eq_conflict = 0\n",
    "conflict = 0\n",
    "\n",
    "for a, (b, need_switch_bc), (c, need_switch_ca) in cycles:\n",
    "    ab = id_df.loc[a]\n",
    "    bc = id_df.loc[b]\n",
    "    ca = id_df.loc[c]\n",
    "    b_b_name = 'b' if need_switch_bc else 'a'\n",
    "    b_c_name = 'a' if need_switch_bc else 'b'\n",
    "    c_c_name = 'b' if need_switch_ca else 'a'\n",
    "    c_a_name = 'a' if need_switch_ca else 'b'\n",
    "    if ab['num_a_wins']:\n",
    "        if bc[f'num_{b_b_name}_wins']:\n",
    "            if ca[f'num_{c_c_name}_wins']:\n",
    "                conflict += 1\n",
    "            elif ca[f'num_{c_a_name}_wins']:\n",
    "                consistent += 1\n",
    "            else:\n",
    "                eq_conflict += 1\n",
    "        elif bc[f'num_{b_c_name}_wins']:\n",
    "            if ca[f'num_{c_c_name}_wins']:\n",
    "                consistent += 1\n",
    "            elif ca[f'num_{c_a_name}_wins']:\n",
    "                consistent += 1\n",
    "            else:\n",
    "                eq_consistent += 1\n",
    "        else:\n",
    "            if ca[f'num_{c_c_name}_wins']:\n",
    "                eq_conflict += 1\n",
    "            elif ca[f'num_{c_a_name}_wins']:\n",
    "                eq_consistent += 1\n",
    "            else:\n",
    "                double_eq_conflict += 1\n",
    "    elif ab['num_b_wins']:\n",
    "        if bc[f'num_{b_b_name}_wins']:\n",
    "            if ca[f'num_{c_a_name}_wins']:\n",
    "                consistent += 1\n",
    "            elif ca[f'num_{c_c_name}_wins']:\n",
    "                consistent += 1\n",
    "            else:\n",
    "                eq_consistent += 1\n",
    "        elif bc[f'num_{b_c_name}_wins']:\n",
    "            if ca[f'num_{c_a_name}_wins']:\n",
    "                conflict += 1\n",
    "            elif ca[f'num_{c_c_name}_wins']:\n",
    "                consistent += 1\n",
    "            else:\n",
    "                eq_conflict += 1\n",
    "        else:\n",
    "            if ca[f'num_{c_a_name}_wins']:\n",
    "                eq_conflict += 1\n",
    "            elif ca[f'num_{c_c_name}_wins']:\n",
    "                eq_consistent += 1\n",
    "            else:\n",
    "                double_eq_conflict += 1\n",
    "    else:\n",
    "        if bc[f'num_{b_b_name}_wins']:\n",
    "            if ca[f'num_{c_a_name}_wins']:\n",
    "                eq_consistent += 1\n",
    "            elif ca[f'num_{c_c_name}_wins']:\n",
    "                eq_conflict += 1\n",
    "            else:\n",
    "                double_eq_conflict += 1\n",
    "        elif bc[f'num_{b_c_name}_wins']:\n",
    "            if ca[f'num_{c_a_name}_wins']:\n",
    "                eq_consistent += 1\n",
    "            elif ca[f'num_{c_c_name}_wins']:\n",
    "                eq_conflict += 1\n",
    "            else:\n",
    "                double_eq_conflict += 1\n",
    "        else:\n",
    "            triple_eq_consistent += 1\n",
    "print(f'Consistent: {consistent}, conflict: {conflict}, unclear: {unclear}, eq consistent: {eq_consistent}, eq conflict: {eq_conflict}, 3-eq consistent: {triple_eq_consistent}, 2-eq conflict: {double_eq_conflict}. Total: {consistent+conflict+unclear+eq_consistent+eq_conflict+double_eq_conflict+triple_eq_consistent} (should be {len(cycles)})')\n",
    "print(f'Consistent total: {consistent+eq_consistent+triple_eq_consistent}, conflict total: {conflict+eq_conflict}, unclear: {unclear+double_eq_conflict}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd expect at least 13/27 ~= 48.15% ratings to be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6613482249700837"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(consistent+eq_consistent+triple_eq_consistent)/len(cycles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only looking at >/< ratings, we'd expect at least 6/8 = 0.75 ratings to be consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8782122905027933"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistent/(consistent+conflict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------\n",
      "all: 589 ratings.\n",
      "Average metrics for a human rater when comparing them to other human raters. Average across 26 raters.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_wins</th>\n",
       "      <th>b_wins</th>\n",
       "      <th>equal_bad</th>\n",
       "      <th>equal_good</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.514857</td>\n",
       "      <td>0.588939</td>\n",
       "      <td>0.270182</td>\n",
       "      <td>0.133577</td>\n",
       "      <td>0.432587</td>\n",
       "      <td>0.376889</td>\n",
       "      <td>0.472159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.491258</td>\n",
       "      <td>0.550100</td>\n",
       "      <td>0.282890</td>\n",
       "      <td>0.146526</td>\n",
       "      <td>0.432587</td>\n",
       "      <td>0.367694</td>\n",
       "      <td>0.432587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.461912</td>\n",
       "      <td>0.522147</td>\n",
       "      <td>0.206201</td>\n",
       "      <td>0.117720</td>\n",
       "      <td>0.432587</td>\n",
       "      <td>0.326995</td>\n",
       "      <td>0.406433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>20.346154</td>\n",
       "      <td>20.230769</td>\n",
       "      <td>10.423077</td>\n",
       "      <td>5.692308</td>\n",
       "      <td>0.432587</td>\n",
       "      <td>56.692308</td>\n",
       "      <td>56.692308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              a_wins     b_wins  equal_bad  equal_good  accuracy  macro avg  \\\n",
       "precision   0.514857   0.588939   0.270182    0.133577  0.432587   0.376889   \n",
       "recall      0.491258   0.550100   0.282890    0.146526  0.432587   0.367694   \n",
       "f1-score    0.461912   0.522147   0.206201    0.117720  0.432587   0.326995   \n",
       "support    20.346154  20.230769  10.423077    5.692308  0.432587  56.692308   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.472159  \n",
       "recall         0.432587  \n",
       "f1-score       0.406433  \n",
       "support       56.692308  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for maximum agreement rater\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_wins</th>\n",
       "      <th>b_wins</th>\n",
       "      <th>equal_bad</th>\n",
       "      <th>equal_good</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.630252</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>0.421529</td>\n",
       "      <td>0.498997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>0.422603</td>\n",
       "      <td>0.510145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.598425</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>0.420867</td>\n",
       "      <td>0.503786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>345.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               a_wins      b_wins  equal_bad  equal_good  accuracy  \\\n",
       "precision    0.575758    0.630252   0.307692    0.172414  0.510145   \n",
       "recall       0.622951    0.625000   0.317460    0.125000  0.510145   \n",
       "f1-score     0.598425    0.627615   0.312500    0.144928  0.510145   \n",
       "support    122.000000  120.000000  63.000000   40.000000  0.510145   \n",
       "\n",
       "            macro avg  weighted avg  \n",
       "precision    0.421529      0.498997  \n",
       "recall       0.422603      0.510145  \n",
       "f1-score     0.420867      0.503786  \n",
       "support    345.000000    345.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------\n",
      "overall: 235 ratings.\n",
      "Average metrics for a human rater when comparing them to other human raters. Average across 12 raters.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_wins</th>\n",
       "      <th>b_wins</th>\n",
       "      <th>equal_bad</th>\n",
       "      <th>equal_good</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.496285</td>\n",
       "      <td>0.158159</td>\n",
       "      <td>0.168442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366448</td>\n",
       "      <td>0.205722</td>\n",
       "      <td>0.322681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.306187</td>\n",
       "      <td>0.362037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366448</td>\n",
       "      <td>0.285316</td>\n",
       "      <td>0.366448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.462463</td>\n",
       "      <td>0.182543</td>\n",
       "      <td>0.226487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366448</td>\n",
       "      <td>0.217873</td>\n",
       "      <td>0.319177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>13.916667</td>\n",
       "      <td>7.083333</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>0.366448</td>\n",
       "      <td>29.666667</td>\n",
       "      <td>29.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              a_wins    b_wins  equal_bad  equal_good  accuracy  macro avg  \\\n",
       "precision   0.496285  0.158159   0.168442    0.000000  0.366448   0.205722   \n",
       "recall      0.473039  0.306187   0.362037    0.000000  0.366448   0.285316   \n",
       "f1-score    0.462463  0.182543   0.226487    0.000000  0.366448   0.217873   \n",
       "support    13.916667  7.083333   7.250000    1.416667  0.366448  29.666667   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.322681  \n",
       "recall         0.366448  \n",
       "f1-score       0.319177  \n",
       "support       29.666667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for maximum agreement rater\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_wins</th>\n",
       "      <th>b_wins</th>\n",
       "      <th>equal_bad</th>\n",
       "      <th>equal_good</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.371871</td>\n",
       "      <td>0.474555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.371212</td>\n",
       "      <td>0.477273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.584071</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.438356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.368510</td>\n",
       "      <td>0.472698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              a_wins     b_wins  equal_bad  equal_good  accuracy   macro avg  \\\n",
       "precision   0.568966   0.518519   0.400000         0.0  0.477273    0.371871   \n",
       "recall      0.600000   0.400000   0.484848         0.0  0.477273    0.371212   \n",
       "f1-score    0.584071   0.451613   0.438356         0.0  0.477273    0.368510   \n",
       "support    55.000000  35.000000  33.000000         9.0  0.477273  132.000000   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.474555  \n",
       "recall         0.477273  \n",
       "f1-score       0.472698  \n",
       "support      132.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------\n",
      "high-flow: 185 ratings.\n",
      "Average metrics for a human rater when comparing them to other human raters. Average across 16 raters.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_wins</th>\n",
       "      <th>b_wins</th>\n",
       "      <th>equal_bad</th>\n",
       "      <th>equal_good</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.305240</td>\n",
       "      <td>0.589673</td>\n",
       "      <td>0.011409</td>\n",
       "      <td>0.225054</td>\n",
       "      <td>0.432655</td>\n",
       "      <td>0.282844</td>\n",
       "      <td>0.454855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.332242</td>\n",
       "      <td>0.583316</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.348958</td>\n",
       "      <td>0.432655</td>\n",
       "      <td>0.347379</td>\n",
       "      <td>0.432655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.266226</td>\n",
       "      <td>0.545997</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.206289</td>\n",
       "      <td>0.432655</td>\n",
       "      <td>0.259836</td>\n",
       "      <td>0.402297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>5.562500</td>\n",
       "      <td>11.187500</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>0.432655</td>\n",
       "      <td>22.062500</td>\n",
       "      <td>22.062500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             a_wins     b_wins  equal_bad  equal_good  accuracy  macro avg  \\\n",
       "precision  0.305240   0.589673   0.011409    0.225054  0.432655   0.282844   \n",
       "recall     0.332242   0.583316   0.125000    0.348958  0.432655   0.347379   \n",
       "f1-score   0.266226   0.545997   0.020833    0.206289  0.432655   0.259836   \n",
       "support    5.562500  11.187500   1.687500    3.625000  0.432655  22.062500   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.454855  \n",
       "recall         0.432655  \n",
       "f1-score       0.402297  \n",
       "support       22.062500  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for maximum agreement rater\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_wins</th>\n",
       "      <th>b_wins</th>\n",
       "      <th>equal_bad</th>\n",
       "      <th>equal_good</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.59292</td>\n",
       "      <td>0.421237</td>\n",
       "      <td>0.562428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.59292</td>\n",
       "      <td>0.433303</td>\n",
       "      <td>0.592920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.59292</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.575842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.59292</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>113.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              a_wins     b_wins  equal_bad  equal_good  accuracy   macro avg  \\\n",
       "precision   0.571429   0.701754        0.0    0.411765   0.59292    0.421237   \n",
       "recall      0.645161   0.754717        0.0    0.333333   0.59292    0.433303   \n",
       "f1-score    0.606061   0.727273        0.0    0.368421   0.59292    0.425439   \n",
       "support    31.000000  53.000000        8.0   21.000000   0.59292  113.000000   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.562428  \n",
       "recall         0.592920  \n",
       "f1-score       0.575842  \n",
       "support      113.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------\n",
      "low-flow: 169 ratings.\n",
      "Average metrics for a human rater when comparing them to other human raters. Average across 15 raters.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_wins</th>\n",
       "      <th>b_wins</th>\n",
       "      <th>equal_bad</th>\n",
       "      <th>equal_good</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.400889</td>\n",
       "      <td>0.463759</td>\n",
       "      <td>0.205820</td>\n",
       "      <td>0.064591</td>\n",
       "      <td>0.41505</td>\n",
       "      <td>0.283765</td>\n",
       "      <td>0.365771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.589899</td>\n",
       "      <td>0.491111</td>\n",
       "      <td>0.224074</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.41505</td>\n",
       "      <td>0.363771</td>\n",
       "      <td>0.415050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.446521</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.138105</td>\n",
       "      <td>0.088681</td>\n",
       "      <td>0.41505</td>\n",
       "      <td>0.275895</td>\n",
       "      <td>0.351428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>6.066667</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>2.466667</td>\n",
       "      <td>0.41505</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             a_wins    b_wins  equal_bad  equal_good  accuracy  macro avg  \\\n",
       "precision  0.400889  0.463759   0.205820    0.064591   0.41505   0.283765   \n",
       "recall     0.589899  0.491111   0.224074    0.150000   0.41505   0.363771   \n",
       "f1-score   0.446521  0.430272   0.138105    0.088681   0.41505   0.275895   \n",
       "support    6.400000  6.066667   3.066667    2.466667   0.41505  18.000000   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.365771  \n",
       "recall         0.415050  \n",
       "f1-score       0.351428  \n",
       "support       18.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for maximum agreement rater\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_wins</th>\n",
       "      <th>b_wins</th>\n",
       "      <th>equal_bad</th>\n",
       "      <th>equal_good</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.385453</td>\n",
       "      <td>0.490217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.410985</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.396413</td>\n",
       "      <td>0.508009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              a_wins     b_wins  equal_bad  equal_good  accuracy   macro avg  \\\n",
       "precision   0.615385   0.648649   0.277778         0.0      0.53    0.385453   \n",
       "recall      0.666667   0.750000   0.227273         0.0      0.53    0.410985   \n",
       "f1-score    0.640000   0.695652   0.250000         0.0      0.53    0.396413   \n",
       "support    36.000000  32.000000  22.000000        10.0      0.53  100.000000   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.490217  \n",
       "recall         0.530000  \n",
       "f1-score       0.508009  \n",
       "support      100.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = {}\n",
    "for task in ['all', 'overall', 'high-flow', 'low-flow']:\n",
    "    if task != 'all':\n",
    "        task_df = df[df['task'] == task]\n",
    "    else:\n",
    "        task_df = df\n",
    "    print(f'\\n--------------------------------------\\n{task}: {task_df.shape[0]} ratings.')\n",
    "\n",
    "    # Compare individual against other raters\n",
    "    true_and_pred_individual = {}\n",
    "    for rater in task_df['user_id'].unique():\n",
    "        true_and_pred_individual[rater] = []\n",
    "        rater_df = task_df[task_df['user_id'] == rater]\n",
    "        other_df = task_df[task_df['user_id'] != rater]\n",
    "        other_groups = other_df.groupby(['model_a', 'model_b', 'start_date', 'objective', 'basin', 'task'])\n",
    "        for idx, rating in rater_df.iterrows():\n",
    "            rater_class = {'a_wins': rating['num_a_wins'], 'b_wins': rating['num_b_wins'], 'equal_good': rating['num_equal_good'], 'equal_bad': rating['num_equal_bad']}\n",
    "            rater_class = max(rater_class, key=rater_class.get)\n",
    "\n",
    "            setting = (rating['model_a'], rating['model_b'], rating['start_date'], rating['objective'], rating['basin'], rating['task'])\n",
    "            if setting not in other_groups.groups:\n",
    "                continue\n",
    "\n",
    "            other_group = other_groups.get_group(setting)\n",
    "            for key, other_rating in other_group.iterrows():\n",
    "                other_class = {'a_wins': other_rating['num_a_wins'], 'b_wins': other_rating['num_b_wins'], 'equal_good': other_rating['num_equal_good'], 'equal_bad': other_rating['num_equal_bad']}\n",
    "                other_class = max(other_class, key=other_class.get)  # Only one other rating, so there can't be a tie\n",
    "                true_and_pred_individual[rater].append((other_class, rater_class))\n",
    "\n",
    "        if len(true_and_pred_individual[rater]) > 0:\n",
    "            true_and_pred_individual[rater] = np.array(true_and_pred_individual[rater])\n",
    "        else:\n",
    "            true_and_pred_individual.pop(rater)\n",
    "\n",
    "    # Compare max rater against individual raters\n",
    "    true_and_pred_max = []\n",
    "    np.random.seed(0)\n",
    "    groupby = task_df.groupby(['model_a', 'model_b', 'start_date', 'objective', 'basin', 'task'])\n",
    "    for idx, rating in task_df.iterrows():\n",
    "        setting = (rating['model_a'], rating['model_b'], rating['start_date'], rating['objective'], rating['basin'], rating['task'])\n",
    "\n",
    "        rater_class = {'a_wins': rating['num_a_wins'], 'b_wins': rating['num_b_wins'], 'equal_good': rating['num_equal_good'], 'equal_bad': rating['num_equal_bad']}\n",
    "        rater_class = max(rater_class, key=rater_class.get)\n",
    "        \n",
    "        group_df = groupby.get_group(setting)\n",
    "        group_df = group_df[group_df['user_id'] != rating['user_id']]  # Exclude rater themself\n",
    "        group_size = group_df.shape[0]\n",
    "        if group_size < 1:\n",
    "            continue\n",
    "        a_wins = group_df['num_a_wins'].sum() / group_size\n",
    "        b_wins = group_df['num_b_wins'].sum() / group_size\n",
    "        equal = (group_df['num_equal_good'].sum() + group_df['num_equal_bad'].sum()) / group_size\n",
    "        equal_good = group_df['num_equal_good'].sum() / group_size\n",
    "        equal_bad =  group_df['num_equal_bad'].sum() / group_size\n",
    "        counts = {'a_wins': a_wins, 'b_wins': b_wins, 'equal_good': equal_good, 'equal_bad': equal_bad}\n",
    "        max_class = max(counts, key=counts.get)\n",
    "        tied_classes = [k for k, v in counts.items() if v == counts[max_class]]\n",
    "        # In the case of a tie between rater_class and another one, decide by coin toss:\n",
    "        if len(tied_classes) > 1:\n",
    "            coin_toss = np.random.randint(len(tied_classes))\n",
    "            max_class = tied_classes[coin_toss]\n",
    "        \n",
    "        true_and_pred_max.append((rater_class, max_class))\n",
    "    true_and_pred_max = np.array(true_and_pred_max)\n",
    "\n",
    "    individual_scores = []\n",
    "    for rater, results in true_and_pred_individual.items():\n",
    "        individual_scores.append(metrics.classification_report(results[:, 0], results[:, 1], zero_division=0, output_dict=True))\n",
    "\n",
    "    # Filter to only include reports where all 4 classes are present (7 = 4 + accuracy + macro avg + weighted avg)\n",
    "    individual_scores = [pd.DataFrame(score) for score in individual_scores if len(score) == 7]\n",
    "    print(f'Average metrics for a human rater when comparing them to other human raters. Average across {len(individual_scores)} raters.')\n",
    "    results_df[(task, 'Individual')] = sum(individual_scores) / len(individual_scores)\n",
    "    display(results_df[(task, 'Individual')])\n",
    "\n",
    "    print('Metrics for maximum agreement rater')\n",
    "    results_df[(task, 'Majority vote')] = pd.DataFrame(metrics.classification_report(true_and_pred_max[:, 0], true_and_pred_max[:, 1], output_dict=True))\n",
    "    display(results_df[(task, 'Majority vote')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llSSSS}\n",
      "\\toprule\n",
      "{} & {} & {a_wins} & {b_wins} & {equal_good} & {equal_bad} \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{2}{*}{f1-score} & Individual & 0.46 & 0.52 & 0.12 & 0.21 \\\\\n",
      " & Majority vote & 0.60 & 0.63 & 0.14 & 0.31 \\\\\n",
      "\\multirow[c]{2}{*}{precision} & Individual & 0.51 & 0.59 & 0.13 & 0.27 \\\\\n",
      " & Majority vote & 0.58 & 0.63 & 0.17 & 0.31 \\\\\n",
      "\\multirow[c]{2}{*}{recall} & Individual & 0.49 & 0.55 & 0.15 & 0.28 \\\\\n",
      " & Majority vote & 0.62 & 0.62 & 0.12 & 0.32 \\\\\n",
      "\\multirow[c]{2}{*}{support} & Individual & 20.35 & 20.23 & 5.69 & 10.42 \\\\\n",
      " & Majority vote & 122.00 & 120.00 & 40.00 & 63.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_df = pd.concat({key: results_df[('all', key)] for key in ['Individual', 'Majority vote']}, axis=0)\n",
    "paper_df.index = paper_df.index.reorder_levels([1,0])\n",
    "latex = paper_df.sort_index()[['a_wins', 'b_wins', 'equal_good', 'equal_bad']].style.format(precision=2).to_latex(convert_css=True, siunitx=True, hrules=True)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2a6f1a0764d418ed202c4d837ddf731754b5106b91308298f70f739593497bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('maps')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
